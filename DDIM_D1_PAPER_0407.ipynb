{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "smBL6yzA2b7v",
    "outputId": "b8733065-e7e4-46d0-c520-63c12d822d7a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hypertension\n",
       "1    51\n",
       "0    48\n",
       "2    21\n",
       "3    12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from abc import abstractmethod\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops import repeat\n",
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 데이터 로드\n",
    "file_path = \"E:/dataset/PPG-BP/combined_dataset.xlsx\"\n",
    "df = pd.read_excel(file_path)  # read_csv -> read_excel로 변경\n",
    "df\n",
    "\n",
    "df['Hypertension'].value_counts()\n",
    "\n",
    "# Normal을 8:2로 나눔 (훈련: 80%, 테스트: 20%)\n",
    "normal_train, normal_test = train_test_split(\n",
    "    df[df[\"Hypertension\"] == \"Normal\"], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 나머지 클래스는 테스트 데이터에만 포함\n",
    "test_data = normal_test.copy()\n",
    "for category in [\"Prehypertension\", \"Stage 1 hypertension\", \"Stage 2 hypertension\"]:\n",
    "    _, test = train_test_split(\n",
    "        df[df[\"Hypertension\"] == category], test_size=0.2, random_state=42\n",
    "    )\n",
    "    test_data = pd.concat([test_data, test])\n",
    "\n",
    "# 훈련 데이터는 Normal만 포함\n",
    "train_data = normal_train.copy()\n",
    "train_data[\"Hypertension\"].value_counts()\n",
    "test_data[\"Hypertension\"].value_counts()\n",
    "\n",
    "ppg_columns = [str(i) for i in range(2091, 2101)]\n",
    "\n",
    "# 'Hypertension'이 0 (Normal)인 데이터만 훈련 데이터로 선택\n",
    "train_data = train_data[train_data[\"Hypertension\"] == 'Normal'][ppg_columns]\n",
    "\n",
    "\n",
    "# 테스트 데이터는 기존과 동일하게 유지 (Hypertension 레이블 + PPG 데이터)\n",
    "test_data = test_data[[\"Hypertension\"] + ppg_columns]\n",
    "\n",
    "test_data['Hypertension'].value_counts()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Label Encoding을 위한 변환기 생성\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Hypertension 컬럼을 Label Encoding 적용\n",
    "test_data[\"Hypertension_Encoded\"] = label_encoder.fit_transform(test_data[\"Hypertension\"])\n",
    "\n",
    "# 기존 Hypertension 컬럼 제거 후 새로운 컬럼으로 대체\n",
    "test_data_encoded = test_data.drop(columns=[\"Hypertension\"]).rename(columns={\"Hypertension_Encoded\": \"Hypertension\"})\n",
    "test_data_encoded['Hypertension'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "QhSP-j7tko9g"
   },
   "outputs": [],
   "source": [
    "def timestep_embedding(timesteps, dim, max_period=10000, repeat_only=False):\n",
    "    \"\"\"\n",
    "    Create sinusoidal timestep embeddings.\n",
    "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
    "                      These may be fractional.\n",
    "    :param dim: the dimension of the output.\n",
    "    :param max_period: controls the minimum frequency of the embeddings.\n",
    "    :return: an [N x dim] Tensor of positional embeddings.\n",
    "    \"\"\"\n",
    "    if not repeat_only:\n",
    "        half = dim // 2\n",
    "        freqs = torch.exp(\n",
    "            -math.log(max_period)\n",
    "            * torch.arange(start=0, end=half, dtype=torch.float32)\n",
    "            / half\n",
    "        ).to(device=timesteps.device)\n",
    "        args = timesteps[:, None].float() * freqs[None]\n",
    "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
    "        if dim % 2:\n",
    "            embedding = torch.cat(\n",
    "                [embedding, torch.zeros_like(embedding[:, :1])], dim=-1\n",
    "            )\n",
    "    else:\n",
    "        embedding = repeat(timesteps, \"b -> b d\", d=dim)\n",
    "    return embedding\n",
    "\n",
    "\n",
    "def zero_module(module):\n",
    "    \"\"\"\n",
    "    Zero out the parameters of a module and return it.\n",
    "    \"\"\"\n",
    "    for p in module.parameters():\n",
    "        p.detach().zero_()\n",
    "    return module\n",
    "\n",
    "\n",
    "class TimestepBlock(nn.Module):\n",
    "    @abstractmethod\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        Apply the module to `x` given `emb` timestep embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n",
    "    \"\"\"\n",
    "    A sequential module that passes timestep embeddings to the children that\n",
    "    support it as an extra input.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x, emb, context=None):\n",
    "        for layer in self:\n",
    "            if isinstance(layer, TimestepBlock):\n",
    "                x = layer(x, emb)  # Pass emb to TimestepBlock layers\n",
    "            else:\n",
    "                x = layer(x)  # Regular layers do not receive emb\n",
    "        return x\n",
    "\n",
    "def Normalize(in_channels):\n",
    "    return nn.GroupNorm(\n",
    "        num_groups=32, num_channels=in_channels, eps=1e-6, affine=True\n",
    "    )\n",
    "\n",
    "\n",
    "def count_flops_attn(model, _x, y):\n",
    "    \"\"\"\n",
    "    A counter for the `thop` package to count the operations in an\n",
    "    attention operation.\n",
    "    Meant to be used like:\n",
    "        macs, params = thop.profile(\n",
    "            model,\n",
    "            inputs=(inputs, timestamps),\n",
    "            custom_ops={QKVAttention: QKVAttention.count_flops},\n",
    "        )\n",
    "    \"\"\"\n",
    "    b, c, *spatial = y[0].shape\n",
    "    num_spatial = int(np.prod(spatial))\n",
    "    # We perform two matmuls with the same number of ops.\n",
    "    # The first computes the weight matrix, the second computes\n",
    "    # the combination of the value vectors.\n",
    "    matmul_ops = 2 * b * (num_spatial**2) * c\n",
    "    model.total_ops += th.DoubleTensor([matmul_ops])\n",
    "\n",
    "\n",
    "class QKVAttentionLegacy(nn.Module):\n",
    "    \"\"\"\n",
    "    A module which performs QKV attention.\n",
    "    Matches legacy QKVAttention + input/ouput heads shaping\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, qkv):\n",
    "        \"\"\"\n",
    "        Apply QKV attention.\n",
    "        :param qkv: an [N x (H * 3 * C) x T] tensor of Qs, Ks, and Vs.\n",
    "        :return: an [N x (H * C) x T] tensor after attention.\n",
    "        \"\"\"\n",
    "        bs, width, length = qkv.shape\n",
    "        assert width % (3 * self.n_heads) == 0\n",
    "        ch = width // (3 * self.n_heads)\n",
    "        q, k, v = qkv.reshape(bs * self.n_heads, ch * 3, length).split(\n",
    "            ch, dim=1\n",
    "        )\n",
    "        scale = 1 / math.sqrt(math.sqrt(ch))\n",
    "        weight = th.einsum(\n",
    "            \"bct,bcs->bts\", q * scale, k * scale\n",
    "        )  # More stable with f16 than dividing afterwards\n",
    "        weight = th.softmax(weight.float(), dim=-1).type(weight.dtype)\n",
    "        a = th.einsum(\"bts,bcs->bct\", weight, v)\n",
    "        return a.reshape(bs, -1, length)\n",
    "\n",
    "    @staticmethod\n",
    "    def count_flops(model, _x, y):\n",
    "        return count_flops_attn(model, _x, y)\n",
    "\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    An attention block that allows spatial positions to attend to each other.\n",
    "    Originally ported from here, but adapted to the N-d case.\n",
    "    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        num_heads=1,\n",
    "        num_head_channels=-1,\n",
    "        use_checkpoint=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        if num_head_channels == -1:\n",
    "            self.num_heads = num_heads\n",
    "        else:\n",
    "            assert channels % num_head_channels == 0, (\n",
    "                f\"q,k,v channels {channels} is \"\n",
    "                f\"not divisible by num_head_channels {num_head_channels}\"\n",
    "            )\n",
    "            self.num_heads = channels // num_head_channels\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.norm = Normalize(channels)\n",
    "        self.qkv = nn.Conv1d(channels, channels * 3, 1)\n",
    "        self.attention = QKVAttentionLegacy(self.num_heads)\n",
    "\n",
    "        self.proj_out = zero_module(nn.Conv1d(channels, channels, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward(\n",
    "            x,\n",
    "        )\n",
    "\n",
    "    def _forward(self, x):\n",
    "        b, c, *spatial = x.shape\n",
    "        x = x.reshape(b, c, -1)\n",
    "        qkv = self.qkv(self.norm(x))\n",
    "        h = self.attention(qkv)\n",
    "        h = self.proj_out(h)\n",
    "        return (x + h).reshape(b, c, *spatial)\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    \"\"\"\n",
    "    A downsampling layer with an optional convolution.\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, use_conv, out_channels=None, padding=1):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.op = nn.Conv1d(\n",
    "                self.channels, self.out_channels, 3, stride=2, padding=padding\n",
    "            )#TODO:Mudar\n",
    "        else:\n",
    "            assert self.channels == self.out_channels\n",
    "            self.op = nn.AvgPool1d(kernel_size=2, stride=2)#TODO: Mudar\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        return self.op(x)\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\"\n",
    "    An upsampling layer with an optional convolution.\n",
    "    :param channels: channels in the inputs and outputs.\n",
    "    :param use_conv: a bool determining if a convolution is applied.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels, use_conv, out_channels=None, padding=1):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        if use_conv:\n",
    "            self.conv = nn.Conv1d(\n",
    "                self.channels, self.out_channels, 3, padding=padding\n",
    "            )#TODO:Mudar\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.shape[1] == self.channels\n",
    "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "        if self.use_conv:\n",
    "            x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResBlock(TimestepBlock):  # Ensure ResBlock inherits from TimestepBlock\n",
    "    def __init__(\n",
    "        self,\n",
    "        channels,\n",
    "        emb_channels,\n",
    "        dropout,\n",
    "        out_channels=None,\n",
    "        use_conv=False,\n",
    "        use_scale_shift_norm=False,\n",
    "        up=False,\n",
    "        down=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.emb_channels = emb_channels\n",
    "        self.dropout = dropout\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.use_scale_shift_norm = use_scale_shift_norm\n",
    "\n",
    "        self.in_layers = nn.Sequential(\n",
    "            Normalize(channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv1d(channels, self.out_channels, 3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.updown = up or down\n",
    "\n",
    "        if up:\n",
    "            self.h_upd = Upsample(channels, False)\n",
    "            self.x_upd = Upsample(channels, False)\n",
    "        elif down:\n",
    "            self.h_upd = Downsample(channels, False)\n",
    "            self.x_upd = Downsample(channels, False)\n",
    "        else:\n",
    "            self.h_upd = self.x_upd = nn.Identity()\n",
    "\n",
    "        self.emb_layers = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_channels,\n",
    "                2 * self.out_channels\n",
    "                if use_scale_shift_norm\n",
    "                else self.out_channels,\n",
    "            ),\n",
    "        )\n",
    "        self.out_layers = nn.Sequential(\n",
    "            Normalize(self.out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            zero_module(\n",
    "                nn.Conv1d(self.out_channels, self.out_channels, 3, padding=1)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if self.out_channels == channels:\n",
    "            self.skip_connection = nn.Identity()\n",
    "        elif use_conv:\n",
    "            self.skip_connection = nn.Conv1d(\n",
    "                channels, self.out_channels, kernel_size=1\n",
    "            )\n",
    "        else:\n",
    "            self.skip_connection = nn.Conv1d(channels, self.out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        \"\"\"\n",
    "        Apply the ResBlock to `x` with timestep embeddings `emb`.\n",
    "        \"\"\"\n",
    "        if self.updown:\n",
    "            in_rest, in_conv = self.in_layers[:-1], self.in_layers[-1]\n",
    "            h = in_rest(x)\n",
    "            h = self.h_upd(h)\n",
    "            x = self.x_upd(x)\n",
    "            h = in_conv(h)\n",
    "        else:\n",
    "            h = self.in_layers(x)\n",
    "\n",
    "        emb_out = self.emb_layers(emb).type(h.dtype)\n",
    "        while len(emb_out.shape) < len(h.shape):\n",
    "            emb_out = emb_out[..., None]\n",
    "\n",
    "        if self.use_scale_shift_norm:\n",
    "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
    "            scale, shift = torch.chunk(emb_out, 2, dim=1)\n",
    "            h = out_norm(h) * (1 + scale) + shift\n",
    "            h = out_rest(h)\n",
    "        else:\n",
    "            h = h + emb_out\n",
    "            h = self.out_layers(h)\n",
    "\n",
    "        return self.skip_connection(x) + h\n",
    "\n",
    "class UNetModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=32,\n",
    "        in_channels=10,\n",
    "        model_channels=32,\n",
    "        out_channels=10,\n",
    "        num_res_blocks=5,  # num_res_blocks를 5로 변경\n",
    "        attention_resolutions=[32, 16, 8, 4],\n",
    "        dropout=0.1,\n",
    "        channel_mult=(1, 2, 4, 8, 16),\n",
    "        num_heads=4,\n",
    "        use_scale_shift_norm=False,\n",
    "        resblock_updown=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.in_channels = in_channels\n",
    "        self.model_channels = model_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.attention_resolutions = attention_resolutions\n",
    "        self.dropout = dropout\n",
    "        self.channel_mult = channel_mult\n",
    "        self.num_heads = num_heads\n",
    "        self.use_scale_shift_norm = use_scale_shift_norm\n",
    "        self.resblock_updown = resblock_updown\n",
    "\n",
    "        time_embed_dim = model_channels * 4\n",
    "        self.time_embed = nn.Sequential(\n",
    "            nn.Linear(model_channels, time_embed_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_embed_dim, time_embed_dim),\n",
    "        )\n",
    "\n",
    "        self.input_blocks = nn.ModuleList([\n",
    "            TimestepEmbedSequential(nn.Conv1d(in_channels, model_channels, 3, padding=1))\n",
    "        ])\n",
    "        input_block_chans = [model_channels]\n",
    "        ch = model_channels\n",
    "        ds = 1\n",
    "\n",
    "        for level, mult in enumerate(channel_mult):\n",
    "            for _ in range(num_res_blocks): # 변경된 num_res_blocks 적용\n",
    "                layers = [ResBlock(ch, time_embed_dim, dropout, out_channels=mult * model_channels, use_scale_shift_norm=use_scale_shift_norm)]\n",
    "                ch = mult * model_channels\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, num_heads=num_heads))\n",
    "                self.input_blocks.append(TimestepEmbedSequential(*layers))\n",
    "                input_block_chans.append(ch)\n",
    "            if level != len(channel_mult) - 1:\n",
    "                out_ch = ch\n",
    "                self.input_blocks.append(TimestepEmbedSequential(Downsample(ch, True, out_channels=out_ch)))\n",
    "                ch = out_ch\n",
    "                input_block_chans.append(ch)\n",
    "                ds *= 2\n",
    "\n",
    "        self.middle_block = TimestepEmbedSequential(\n",
    "            ResBlock(ch, time_embed_dim, dropout, use_scale_shift_norm=use_scale_shift_norm),\n",
    "            AttentionBlock(ch, num_heads=num_heads),\n",
    "            ResBlock(ch, time_embed_dim, dropout, use_scale_shift_norm=use_scale_shift_norm),\n",
    "        )\n",
    "\n",
    "        self.output_blocks = nn.ModuleList([])\n",
    "        for level, mult in list(enumerate(channel_mult))[::-1]:\n",
    "            for i in range(num_res_blocks + 1): # 변경된 num_res_blocks 적용\n",
    "                ich = input_block_chans.pop()\n",
    "                layers = [ResBlock(ch + ich, time_embed_dim, dropout, out_channels=model_channels * mult, use_scale_shift_norm=use_scale_shift_norm)]\n",
    "                ch = model_channels * mult\n",
    "                if ds in attention_resolutions:\n",
    "                    layers.append(AttentionBlock(ch, num_heads=num_heads))\n",
    "                if level and i == num_res_blocks:\n",
    "                    out_ch = ch\n",
    "                    layers.append(Upsample(ch, True, out_channels=out_ch))\n",
    "                    ds //= 2\n",
    "                self.output_blocks.append(TimestepEmbedSequential(*layers))\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            Normalize(ch),\n",
    "            nn.SiLU(),\n",
    "            zero_module(nn.Conv1d(ch, out_channels, 3, padding=1)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, timesteps=None, context=None, y=None):\n",
    "        assert timesteps is not None, \"timesteps must be provided\"\n",
    "        hs = []\n",
    "        t_emb = timestep_embedding(timesteps, self.model_channels, repeat_only=False)\n",
    "        emb = self.time_embed(t_emb)\n",
    "\n",
    "        h = x\n",
    "        for module in self.input_blocks:\n",
    "            h = module(h, emb, context)\n",
    "            hs.append(h)\n",
    "        h = self.middle_block(h, emb, context)\n",
    "\n",
    "        for module in self.output_blocks:\n",
    "            h_pop = hs.pop()\n",
    "            if h.shape[2] != h_pop.shape[2]:\n",
    "                h_pop = F.interpolate(h_pop, size=h.shape[2], mode='nearest')\n",
    "            h = torch.cat([h, h_pop], dim=1)\n",
    "            h = module(h, emb, context)\n",
    "\n",
    "        return self.out(h)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    beta_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beta_start, beta_end, timesteps, dtype=torch.float32)\n",
    "\n",
    "def register_schedule(self, beta_schedule, timesteps, linear_start, linear_end):\n",
    "    betas = beta_schedule(timesteps)\n",
    "    alphas = 1. - betas\n",
    "    alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "    alphas_cumprod_prev = torch.cat([torch.ones(1), alphas_cumprod[:-1]])\n",
    "    self.register_buffer('betas', betas)\n",
    "    self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "    self.register_buffer('alphas_cumprod_prev', alphas_cumprod_prev)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "class DDIM(nn.Module):\n",
    "    def __init__(self, unet_config, timesteps, ddim_steps, parameterization='eps', clip_denoised=True, original_elbo_weight=0.0):\n",
    "        super().__init__()\n",
    "        self.unet_config = unet_config\n",
    "        self.timesteps = timesteps\n",
    "        self.ddim_steps = ddim_steps\n",
    "        self.parameterization = parameterization\n",
    "        self.model = UNetModel(**unet_config.get(\"params\", {}))\n",
    "        self.clip_denoised = clip_denoised\n",
    "        self.original_elbo_weight = original_elbo_weight\n",
    "        self.register_schedule = register_schedule.__get__(self) # register_schedule 연결\n",
    "        linear_start = 1e-4 # linear_start 정의\n",
    "        linear_end = 0.02 # linear_end 정의\n",
    "        beta_schedule = linear_beta_schedule # beta_schedule 정의\n",
    "        self.register_schedule(beta_schedule, timesteps, linear_start, linear_end) # register_schedule 호출\n",
    "        self.num_timesteps = int(timesteps)\n",
    "\n",
    "    def register_schedule(self, beta_schedule, timesteps, linear_start, linear_end):\n",
    "        betas = np.linspace(linear_start, linear_end, timesteps, dtype=np.float64)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = np.cumprod(alphas, axis=0)\n",
    "        alphas_cumprod_prev = np.append(1.0, alphas_cumprod[:-1])\n",
    "\n",
    "        self.num_timesteps = int(timesteps)\n",
    "        to_torch = partial(torch.tensor, dtype=torch.float32)\n",
    "\n",
    "        self.register_buffer(\"betas\", to_torch(betas))\n",
    "        self.register_buffer(\"alphas_cumprod\", to_torch(alphas_cumprod))\n",
    "        self.register_buffer(\"alphas_cumprod_prev\", to_torch(alphas_cumprod_prev))\n",
    "        self.register_buffer(\"sqrt_recip_alphas_cumprod\", to_torch(np.sqrt(1.0 / alphas_cumprod)))\n",
    "        self.register_buffer(\"sqrt_one_minus_alphas_cumprod\", to_torch(np.sqrt(1.0 - alphas_cumprod)))\n",
    "\n",
    "    def ddim_sample(self, x, t, eta=0.0):\n",
    "        model_output = self.model(x, t)\n",
    "\n",
    "        if self.parameterization == \"eps\":\n",
    "            pred_x0 = extract(self.sqrt_recip_alphas_cumprod, t, x.shape) * x - \\\n",
    "                      extract(self.sqrt_one_minus_alphas_cumprod, t, x.shape) * model_output\n",
    "        else:\n",
    "            pred_x0 = model_output\n",
    "\n",
    "        if self.clip_denoised:\n",
    "            pred_x0 = torch.clamp(pred_x0, -1.0, 1.0)\n",
    "\n",
    "        sigma = eta * (1 - extract(self.alphas_cumprod, t, x.shape)).sqrt()\n",
    "        noise = torch.randn_like(x)\n",
    "\n",
    "        return pred_x0 + sigma * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop_ddim(self, shape, eta=0.0):\n",
    "        device = self.betas.device\n",
    "        img = torch.randn(shape, device=device)\n",
    "\n",
    "        for i in tqdm(reversed(range(0, self.ddim_steps)), desc=\"DDIM sampling\"):\n",
    "            t = torch.full((shape[0],), i, device=device, dtype=torch.long)\n",
    "            img = self.ddim_sample(img, t, eta)\n",
    "\n",
    "        return img\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size=16, eta=0.0):\n",
    "        image_size = self.model.image_size\n",
    "        channels = self.model.in_channels\n",
    "        return self.p_sample_loop_ddim((batch_size, channels, image_size), eta=eta)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        if self.parameterization == 'eps':\n",
    "            pred = self.model(x, t)\n",
    "        elif self.parameterization == 'v':\n",
    "            pred = self.model(x, t)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown parameterization: {self.parameterization}\")\n",
    "        return pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rznWkAeIomfF",
    "outputId": "6f5b4acd-9e33-4f88-83fe-5a1621c2a9f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|                                                                                | 0/6 [00:00<?, ?it/s]C:\\ProgramData\\anaconda33\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([32, 10, 1])) that is different to the input size (torch.Size([32, 10, 4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Epoch 1/20: 100%|████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  6.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Avg Train Loss = 4801429.8333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|████████████████████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  5.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Avg Train Loss = 4801218.9167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  8.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Avg Train Loss = 4801011.2083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  9.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Avg Train Loss = 4800797.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  8.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Avg Train Loss = 4800583.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Avg Train Loss = 4800366.1667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Avg Train Loss = 4800146.9167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Avg Train Loss = 4799917.5417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 11.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Avg Train Loss = 4799687.8333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 11.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Avg Train Loss = 4799445.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 11.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Avg Train Loss = 4799195.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 11.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Avg Train Loss = 4798936.1667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 11.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Avg Train Loss = 4798681.0833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Avg Train Loss = 4798427.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 11.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Avg Train Loss = 4798174.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 11.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Avg Train Loss = 4797923.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Avg Train Loss = 4797673.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 11.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Avg Train Loss = 4797424.0833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 10.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Avg Train Loss = 4797174.1667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|███████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 11.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Avg Train Loss = 4796923.4167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import optim # Import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "# 데이터셋 로드\n",
    "batch_size = 32\n",
    "\n",
    "# train_data_tensor는 (192, 10) 크기의 입력 데이터\n",
    "train_data_tensor = torch.tensor(train_data.values, dtype=torch.float32)  # (192, 10)\n",
    "train_dataset = TensorDataset(train_data_tensor)  # 레이블 없음\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# test_data_tensor는 (132, 10) 크기의 입력 데이터, test_labels_tensor는 (132,) 크기의 라벨\n",
    "test_data_tensor = torch.tensor\n",
    "\n",
    "# 데이터셋 로드\n",
    "batch_size = 32\n",
    "\n",
    "# train_data_tensor는 (192, 10) 크기의 입력 데이터\n",
    "train_data_tensor = torch.tensor(train_data.values, dtype=torch.float32)  # (192, 10)\n",
    "train_dataset = TensorDataset(train_data_tensor)  # 레이블 없음\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# test_data_tensor는 (132, 10) 크기의 입력 데이터, test_labels_tensor는 (132,) 크기의 라벨\n",
    "test_data_tensor = torch.tensor(test_data_encoded.drop(columns=[\"Hypertension\"]).values, dtype=torch.float32)  # (132, 10)\n",
    "test_labels_tensor = torch.tensor(test_data_encoded[\"Hypertension\"].values, dtype=torch.float32).unsqueeze(-1)  # (132, 1)\n",
    "test_dataset = TensorDataset(test_data_tensor, test_labels_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 학습 함수\n",
    "def train_ddim_model(model, train_loader, test_loader, num_epochs=20, learning_rate=1e-4, device='cuda'):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            x = batch[0].to(device)  # 레이블이 없으므로 x만 가져옴\n",
    "            # x = x.unsqueeze(1)  # Remove this line - it's causing the error\n",
    "\n",
    "            # Reshape x to have the expected shape (batch_size, in_channels, image_size)\n",
    "            x = x.view(x.shape[0], unet_config[\"params\"][\"in_channels\"], -1)\n",
    "\n",
    "            t = torch.randint(0, model.num_timesteps, (x.shape[0],), device=device).long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_fn(model(x, t), x)  # 모델의 출력과 입력 비교 (재구성 손실)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}: Avg Train Loss = {avg_loss:.4f}\")\n",
    "\n",
    "# 모델 초기화\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "unet_config = {\n",
    "    \"params\": {\n",
    "        \"image_size\": 10,  # feature 개수와 맞춤\n",
    "        \"in_channels\": 10,\n",
    "        \"model_channels\": 32,\n",
    "        \"out_channels\": 10,  # feature 개수와 동일하게 설정\n",
    "        \"num_res_blocks\": 2,\n",
    "        \"attention_resolutions\": [5],\n",
    "        \"dropout\": 0.1,\n",
    "        \"channel_mult\": (2, 4, 8),\n",
    "        \"num_heads\": 4,\n",
    "        \"use_scale_shift_norm\": False,\n",
    "        \"resblock_updown\": True,\n",
    "    }\n",
    "}\n",
    "\n",
    "# DDIM 객체 생성 (timesteps가 초기화됨)\n",
    "ddim_model = DDIM(unet_config=unet_config, timesteps=5000, ddim_steps=1000, parameterization='eps').to(device)\n",
    "\n",
    "# 학습 시작\n",
    "EPOCHES = 20  # 변수 정의\n",
    "train_ddim_model(ddim_model, train_loader, test_loader, num_epochs=EPOCHES, learning_rate=1e-4, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wAPx9lHQwEt7",
    "outputId": "c7ed1abe-2b6f-48ef-d3ce-3e453c57d312"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([132, 10])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Model:   0%|                                                                             | 0/5 [00:00<?, ?it/s]C:\\ProgramData\\anaconda33\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([32, 10, 4])) that is different to the input size (torch.Size([32, 10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "C:\\ProgramData\\anaconda33\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([4, 10, 4])) that is different to the input size (torch.Size([4, 10, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Testing Model: 100%|█████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 47.89it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "def test_ddim_model(model, test_loader, device='cuda'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    results = []  # 결과 저장을 위한 리스트\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing Model\"):\n",
    "            x, y = batch  # x는 입력 데이터, y는 실제 라벨 (Hypertension 값)\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # Reshape x to have the expected shape (batch_size, in_channels, image_size)\n",
    "            x = x.view(x.shape[0], unet_config[\"params\"][\"in_channels\"], -1)  # Reshape to match in_channels of UNet\n",
    "\n",
    "            t = torch.randint(0, model.num_timesteps, (x.shape[0],), device=device).long()\n",
    "            pred = model(x, t)\n",
    "\n",
    "            loss = loss_fn(x, pred)\n",
    "\n",
    "            # 결과 저장\n",
    "            for i in range(x.shape[0]):\n",
    "                results.append({\n",
    "                    'Reconstruction_Loss': loss.item(),\n",
    "                    'Label': y[i].item()\n",
    "                })\n",
    "\n",
    "    # 데이터프레임 생성\n",
    "    df_results = pd.DataFrame(results)\n",
    "\n",
    "    return df_results\n",
    "\n",
    "# 테스트 실행\n",
    "df_results = test_ddim_model(ddim_model, test_loader, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Reconstruction_Loss</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4704664.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4704664.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4704664.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4704664.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4704664.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>4037861.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>4792747.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>4792747.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>4792747.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>4792747.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>132 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Reconstruction_Loss  Label\n",
       "0              4704664.0    0.0\n",
       "1              4704664.0    0.0\n",
       "2              4704664.0    0.0\n",
       "3              4704664.0    0.0\n",
       "4              4704664.0    0.0\n",
       "..                   ...    ...\n",
       "127            4037861.5    3.0\n",
       "128            4792747.0    3.0\n",
       "129            4792747.0    3.0\n",
       "130            4792747.0    3.0\n",
       "131            4792747.0    3.0\n",
       "\n",
       "[132 rows x 2 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "M7NcyGcF0dp3",
    "outputId": "99cbd11b-11ce-47e7-cc4a-8e80ef881a0a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>4659101.5</td>\n",
       "      <td>4704664.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4037861.5</td>\n",
       "      <td>4659101.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>4037861.5</td>\n",
       "      <td>4037861.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4037861.5</td>\n",
       "      <td>4792747.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label        min        max\n",
       "0    0.0  4659101.5  4704664.0\n",
       "1    1.0  4037861.5  4659101.5\n",
       "2    2.0  4037861.5  4037861.5\n",
       "3    3.0  4037861.5  4792747.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label별 최소값과 최대값 계산\n",
    "MinMax_result = df_results.groupby('Label')['Reconstruction_Loss'].agg(['min', 'max']).reset_index()\n",
    "MinMax_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "-Gz5lRLD0yF4"
   },
   "outputs": [],
   "source": [
    "threshold= 4659101.00\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "T_z-GVbYwXt1",
    "outputId": "f1126454-773a-4420-ef92-a873f54509b8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Documents\\ESTsoft\\CreatorTemp\\ipykernel_8916\\1676568694.py:26: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  ax = sns.boxplot(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df_results2 = df_results.copy()\n",
    "# 색상 설정\n",
    "color_palette = [\"blue\", \"pink\", \"orange\", \"red\"]\n",
    "\n",
    "# 레이블 및 순서 정의\n",
    "label_mapping = {\n",
    "    0: \"Normal\",\n",
    "    1: \"Prehypertension\",\n",
    "    2: \"Stage 1 hypertension\",\n",
    "    3: \"Stage 2 hypertension\"\n",
    "}\n",
    "label_order = [\"Normal\", \"Prehypertension\", \"Stage 1 hypertension\", \"Stage 2 hypertension\"]\n",
    "\n",
    "# 레이블을 문자열로 변환 및 순서 지정\n",
    "df_results2[\"Label\"] = df_results2[\"Label\"].map(label_mapping)\n",
    "df_results2[\"Label\"] = pd.Categorical(df_results2[\"Label\"], categories=label_order, ordered=True)\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(4, 6))\n",
    "ax = sns.boxplot(\n",
    "    y=df_results['Label'],\n",
    "    x=df_results['Reconstruction_Loss'],\n",
    "    orient='h',\n",
    "    palette=color_palette\n",
    ")\n",
    "\n",
    "# 임계값 표시 (연두색)\n",
    "plt.axvline(threshold, color='limegreen', linestyle='--', linewidth=2, label=f\"Threshold ({threshold})\")\n",
    "\n",
    "# 라벨 및 제목 추가\n",
    "plt.xlabel(\"Reconstruction Loss\")\n",
    "plt.ylabel(\"Label\")\n",
    "plt.title(\"Reconstruction Loss Distribution by Label\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "\n",
    "# 그래프 표시\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8649\n",
      "Recall: 0.7619\n",
      "Precision: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'F1 Score': 0.8648648648648649,\n",
       " 'Recall': 0.7619047619047619,\n",
       " 'Precision': 1.0}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_anomaly_detection(anomaly_df, threshold):\n",
    "    # True_Label을 이진 분류(0과 나머지)로 변환\n",
    "    anomaly_df['Binary_Label'] = (anomaly_df['Label'] != 0).astype(int)\n",
    "    \n",
    "    # 예측값 (Reconstruction_Error가 threshold보다 크면 1, 아니면 0)\n",
    "    anomaly_df['Predicted_Label'] = (anomaly_df['Reconstruction_Loss'] < threshold).astype(int)\n",
    "    \n",
    "    # 실제값\n",
    "    y_true = anomaly_df['Binary_Label']\n",
    "    y_pred = anomaly_df['Predicted_Label']\n",
    "    \n",
    "    # 성능 평가 지표 계산\n",
    "    f1 = f1_score(y_true, y_pred, average='binary')\n",
    "    recall = recall_score(y_true, y_pred, average='binary')\n",
    "    precision = precision_score(y_true, y_pred, average='binary')\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    \n",
    "    return {\n",
    "        'F1 Score': f1,\n",
    "        'Recall': recall,\n",
    "        'Precision': precision,\n",
    "    }\n",
    "\n",
    "evaluate_anomaly_detection(df_results, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
